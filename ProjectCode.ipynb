{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3g5Jn8ykoHw"
      },
      "source": [
        "#Importing all necessary and important libraries that would be used in this project\n",
        "!pip install -U -q PyDrive\n",
        "!pip install -U sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install wandb\n",
        "!pip install pytorch_lightning\n",
        "!pip install matplotlib\n",
        "!wandb login  #Requires the user to create an account on Weights & Biases tool\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import shutil\n",
        "import glob\n",
        "import pickle\n",
        "import collections\n",
        "import math\n",
        "import random\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, LongformerTokenizerFast, AutoModelForQuestionAnswering\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFVunOqi6bDC"
      },
      "source": [
        "#Initializing tokenizer and QA Longformer models that would be used in the subsequent cells/functions\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#LOAD INFORMATION RETRIEVAL MODEL - this ranks passages in a larger context in order to construct a smaller context\n",
        "model_ir = SentenceTransformer('distilroberta-base-msmarco-v2') #Distilroberta trained on MS MARCO (open-domain QA dataset)\n",
        "model_ir = model_ir.to(device)\n",
        "#LOAD QUESTION ANSWERING MODEL - this performs the traditional QA task, which right now is span return\n",
        "model_name_or_path = \"mrm8488/longformer-base-4096-finetuned-squadv2\" #Longformer trained on SQUaD 2.0\n",
        "model_qa = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)\n",
        "tokenizer_qa = LongformerTokenizerFast.from_pretrained(model_name_or_path)\n",
        "model_qa = model_qa.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUFW0DPL67FL"
      },
      "source": [
        "#This function allows colab to hook up Google Drive and use it as local storage\n",
        "def setup():\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  print(\"Step 1. Authenticating Google account user\")\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  #Authenticate access to Google drive\n",
        "  print(\"Step 2. Authenticating access to Google drive\")\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.chdir(\"/content/drive/My Drive\")\n",
        "  directory_path = os.getcwd()+\"/RulesDataset\"\n",
        "  if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "  else:\n",
        "    shutil.rmtree(directory_path)\n",
        "    os.makedirs(directory_path)\n",
        "  os.chdir(directory_path)\n",
        "  #Upload zip file containing .txt files\n",
        "  print(\"Step 3. Please upload only two files: InitialTest.zip and saved_csv_dict.p\")\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSEFdWST64nr"
      },
      "source": [
        "#This function reads in the files uploaded in setup() and imports the finetuning data into variables\n",
        "def process_data():\n",
        "  with zipfile.ZipFile(\"InitialTest.zip\", \"r\") as zip_file:\n",
        "    zip_file.extractall()\n",
        "  data_dict={}\n",
        "  directory_path = \"/content/drive/My Drive/RulesDataset/InitialTest\"\n",
        "  filepaths = glob.glob(directory_path+\"/*.txt\")\n",
        "  filenames = os.listdir(directory_path)\n",
        "  for filepath, filename in zip(filepaths, filenames):\n",
        "    with open(filepath, 'r') as file:\n",
        "      data_content = file.read().splitlines()\n",
        "    data_content = list(filter(None, data_content))\n",
        "    data_dict[filename.split('.')[0]] = data_content\n",
        "  csv_dict = pickle.load(open(\"saved_csv_dict.p\", \"rb\"))\n",
        "  return data_dict, csv_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzMk5Qu_7CQU"
      },
      "source": [
        "#This function returns the word embeddings of the context (rules in the rulebook)\n",
        "def get_context_embeddings(context_sentences):\n",
        "  context_embeddings = torch.from_numpy(model_ir.encode(context_sentences))\n",
        "  return context_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luk-zAa27Stj"
      },
      "source": [
        "#This function reduces the size of the context by selecting top k passages in the context and concatenates them\n",
        "def get_final_context_strings(questions, answers, context):\n",
        "  top_k = 3 # Choose number of top passages to concatenate. Typical is 3 for Longformer.\n",
        "  context_embeddings = get_context_embeddings(context)\n",
        "  num_questions = len(questions)\n",
        "  num_answers = len(answers)\n",
        "  final_contexts = [[] for i in range(num_questions)]\n",
        "  final_context_strings = [\"\" for i in range(num_questions)]\n",
        "  top_k_sentences = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    question_embeddings = torch.from_numpy(model_ir.encode(questions)) # -> Tensor, Size: Number of Questions x Embedding Size\n",
        "  cos_sim_sentences = util.pytorch_cos_sim(question_embeddings, context_embeddings) # Use cosine simularity to rate sentences. -> Tensor, Size: Number of Questions x Number of Passages in Context\n",
        "  top_k_sentences = torch.topk(cos_sim_sentences,top_k)[1] #Indices Tensor (Size: Number of Questions x Indices of Top k Values in Context) - Discard values tensor at index 0\n",
        "  sorted_indices = torch.sort(top_k_sentences,dim=-1)[0] ## Sort sentences into document order to preserve structure, removing extra indices tensor from sort fxn. -> Indices Tensor (Size: Number of Questions x Indices of Top k Values in Context)\n",
        "  sorted_indices_list = sorted_indices.tolist()\n",
        "\n",
        "  for qxn in range(num_questions): # Build all the final contexts\n",
        "    for chunk in sorted_indices_list[qxn]:\n",
        "      final_contexts[qxn].append(sentences[chunk])\n",
        "    final_context_strings[qxn] = \" \".join(final_contexts[qxn])\n",
        "  \n",
        "  return final_context_strings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANMlU1ybFwys"
      },
      "source": [
        "#Naive search for answer span start and end characters in a batch. Not reliable versus multiple possible span returns, but the answer spans in the fine-tuning set only have on possible gold string.\n",
        "\n",
        "def get_answer_spans(questions, answers, final_context_strings):\n",
        "  original_context_embeddings = tokenizer_qa(questions,final_context_strings,return_tensors='pt',padding=True,truncation=True) # Tokenize the input batch using the QA model. Add padding to make all the same size.\n",
        "  num_questions = len(questions)\n",
        "  num_answers = len(answers)\n",
        "  ans_char_starts = [[] for i in range(num_answers)] # Character index for answer start\n",
        "  ans_char_ends = [[] for i in range(num_answers)] # Character index for answer end\n",
        "  ans_starts = [[] for i in range(num_answers)] # Token index for answer start\n",
        "  ans_ends = [[] for i in range(num_answers)] # Token index for answer end\n",
        "  \n",
        "  for i in range(num_questions):\n",
        "    answer_length = len(answers[i])\n",
        "    if final_context_strings[i].find(answers[i]) == -1: # If the answer is no longer in the final span, set correct answer to CLS.\n",
        "      ans_starts[i] = 0\n",
        "      ans_ends[i] = 1\n",
        "    else: # If the answer is still in the span, record the start and end characters of the span.\n",
        "      ans_char_starts[i] = final_context_strings[i].find(answers[i]) # Get character index for start.\n",
        "      ans_char_ends[i] = ans_char_starts[i] + answer_length # Get character index for end.\n",
        "      question_len = len(tokenizer_qa(questions[i])['input_ids'])\n",
        "      ans_starts[i] = (original_context_embeddings.char_to_token(i,ans_char_starts[i])) - question_len # Extracts token index for answer start\n",
        "      ans_ends[i] = (original_context_embeddings.char_to_token(i,ans_char_ends[i] - 1)) - question_len + 1 # Extracts token index for answer end\n",
        "  \n",
        "  return ans_starts, ans_ends"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRR6xbcGs0ml"
      },
      "source": [
        "#This function computes F1 score between the gold sentence and the predicted sentence (sourced from SQuAD)\n",
        "def compute_f1(a_gold, a_pred): \n",
        "  gold_toks = tokenizer_qa(a_gold)\n",
        "  pred_toks = tokenizer_qa(a_pred)\n",
        "  common = collections.Counter(gold_toks['input_ids']) & collections.Counter(pred_toks['input_ids'])\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks['input_ids']) == 0:\n",
        "    return int(gold_toks['input_ids'] == pred_toks['input_ids'])\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks['input_ids'])\n",
        "  recall = 1.0 * num_same / len(gold_toks['input_ids'])\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds07HT6CJpCs"
      },
      "source": [
        "#Performs split on the rulebook data into training, evaluation and test with 80/10/10 split ratio\n",
        "random.seed(123) #for reproducing results\n",
        "def three_split(questions):\n",
        "  num_questions = len(questions)\n",
        "  question_ids = [x for x in range(num_questions)]\n",
        "  templen = num_questions\n",
        "  trainlen = math.ceil(float(0.8 * templen))\n",
        "  templen = templen - trainlen\n",
        "  vallen = math.ceil(float(0.5 * templen))\n",
        "\n",
        "  train_ids = random.sample(question_ids, trainlen)\n",
        "  for i in train_ids:\n",
        "    question_ids.remove(i)\n",
        "  \n",
        "  val_ids = random.sample(question_ids, vallen)\n",
        "  for i in val_ids:\n",
        "    question_ids.remove(i)\n",
        "\n",
        "  test_ids = question_ids\n",
        "\n",
        "  return train_ids, val_ids, test_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGcOP7XcSxxy"
      },
      "source": [
        "#This function computes and prints the bi-gram statistics for a given input list of questions \n",
        "def compute_stats(list_of_questions):\n",
        "  keys = [\"Pandemic\", \"Twilight_Struggle\", \"Terra_Mystica\", \"Food_Chain_Magnate\", \"Great_Western_Trail\", \"Catan\", \"Carcassonne\", \"Terraforming_Mars\", \"Power_Grid_Recharged\"]\n",
        "\n",
        "  word_dict = {}\n",
        "  for question in list_of_questions:\n",
        "    words = question.split()\n",
        "    first = words[0]\n",
        "    second = words[1]\n",
        "\n",
        "    key = first+\"-\"+second\n",
        "\n",
        "    if key in word_dict:\n",
        "      word_dict[key] = word_dict[key] + 1\n",
        "    else:\n",
        "      word_dict[key] = 1\n",
        "\n",
        "  word_dict_desc = dict(sorted(word_dict.items(), key=operator.itemgetter(1),reverse=True))\n",
        "\n",
        "  print(\"The bigram statistics are:\\n\")\n",
        "  for word, count in word_dict_desc.items():\n",
        "    print(word, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6gySrEm3mr4"
      },
      "source": [
        "setup() #Perform setup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5GhHByW3njw"
      },
      "source": [
        "context_dict, csv_dict = process_data() #Read in the Rulebook dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUGTaWcx4Tke"
      },
      "source": [
        "#Reducing size of context for each (question, answer, context) triplet in the dataset\n",
        "keys = [\"Pandemic\", \"Twilight_Struggle\", \"Terra_Mystica\", \"Food_Chain_Magnate\", \"Great_Western_Trail\", \"Catan\", \"Carcassonne\", \"Terraforming_Mars\", \"Power_Grid_Recharged\"]\n",
        "\n",
        "for key in keys:\n",
        "  print(\"Processing: \",key)\n",
        "  sentences = context_dict[key]\n",
        "  questions = csv_dict[key][0]\n",
        "  answers = csv_dict[key][1]\n",
        "  final_context_strings = get_final_context_strings(questions, answers, sentences)\n",
        "  csv_dict[key].append(final_context_strings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA7UJa_94VY_"
      },
      "source": [
        "#Computing answer spans for each (question, answer, context) triplet in the dataset\n",
        "for key in keys:\n",
        "  print(\"Processing: \",key)\n",
        "  questions = csv_dict[key][0]\n",
        "  answers = csv_dict[key][1]\n",
        "  final_context_strings = csv_dict[key][2]\n",
        "  ans_starts, ans_ends = get_answer_spans(questions, answers, final_context_strings)\n",
        "  csv_dict[key].append(ans_starts)\n",
        "  csv_dict[key].append(ans_ends)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8crqv8WmB3Du"
      },
      "source": [
        "#Splitting the dataset into training, evaluation and test sets\n",
        "for key in keys:\n",
        "  print(\"Processing: \",key)\n",
        "  questions = csv_dict[key][0]\n",
        "  train_ids, val_ids, test_ids = three_split(questions)\n",
        "  csv_dict[key].append(train_ids)\n",
        "  csv_dict[key].append(val_ids)\n",
        "  csv_dict[key].append(test_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G__i0uQcP8c0"
      },
      "source": [
        "In csv_dict : 0 - questions, 1 - answers, 2 - final context strings, 3 - ans_start, 4 - ans_end, 5 - train_ids, 6 - val_ids, 7 - test_ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loEek01LJOA8"
      },
      "source": [
        "#Creating the actual train, evaluation and test dataset splits\n",
        "\n",
        "#Preparing list of training questions and their contexts using train_labels\n",
        "train_questions = []\n",
        "train_contexts = []\n",
        "train_answers = []\n",
        "\n",
        "#Preparing list of validation questions and their contexts using eval_labels \n",
        "eval_questions = []\n",
        "eval_contexts = []\n",
        "eval_answers = []\n",
        "\n",
        "#Preparing list of test questions and their contexts using test_labels \n",
        "test_questions = []\n",
        "test_contexts = []\n",
        "test_answers = []\n",
        "\n",
        "for key in keys:\n",
        "  print(\"Processing: \",key)\n",
        "  questions = csv_dict[key][0]\n",
        "  answers = csv_dict[key][1]\n",
        "  contexts = csv_dict[key][2]\n",
        "  ans_start = csv_dict[key][3]\n",
        "  ans_end = csv_dict[key][4]\n",
        "  train_labels = csv_dict[key][5]\n",
        "  eval_labels = csv_dict[key][6]\n",
        "  test_labels = csv_dict[key][7]\n",
        "\n",
        "  for i in train_labels:\n",
        "    train_questions.append(questions[i])\n",
        "    train_contexts.append(contexts[i])\n",
        "    train_answers.append({\"text\": answers[i], 'answer_start': ans_start[i],'answer_end':ans_end[i]})\n",
        "  \n",
        "  for i in eval_labels:\n",
        "    eval_questions.append(questions[i])\n",
        "    eval_contexts.append(contexts[i])\n",
        "    eval_answers.append({\"text\": answers[i], 'answer_start': ans_start[i], 'answer_end':ans_end[i]})\n",
        "  \n",
        "  for i in test_labels:\n",
        "    test_questions.append(questions[i])\n",
        "    test_contexts.append(contexts[i])\n",
        "    test_answers.append({\"text\": answers[i], 'answer_start': ans_start[i], 'answer_end':ans_end[i]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-tApywDPz_9"
      },
      "source": [
        "#Obtain encodings for training and evaluation datasets\n",
        "train_encodings = tokenizer_qa(train_contexts, train_questions, truncation=True, padding=True)\n",
        "eval_encodings = tokenizer_qa(eval_contexts, eval_questions, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfmMGoKgSmdG"
      },
      "source": [
        "#This function adds token positions to the encodings based on dataset type\n",
        "def add_token_positions(encodings, label_type):\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  for key in keys:\n",
        "    print(\"Processing: \",key)\n",
        "    ans_start = csv_dict[key][3]\n",
        "    ans_end = csv_dict[key][4]\n",
        "    labels = csv_dict[key][label_type]\n",
        "    for i in labels:\n",
        "      start_positions.append(ans_start[i])\n",
        "      end_positions.append(ans_end[i])\n",
        "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKC3C1zQWJZO"
      },
      "source": [
        "#Defining custom class for the Rulebook Dataset\n",
        "class BoardGameDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soEMD5kTWL0x"
      },
      "source": [
        "#Adding token position to training and evaluation dataset encodings and also creating the training and evaluation BoardGameDataset objects\n",
        "add_token_positions(train_encodings, 5)\n",
        "add_token_positions(eval_encodings, 6)\n",
        "\n",
        "train_dataset = BoardGameDataset(train_encodings)\n",
        "eval_dataset = BoardGameDataset(eval_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuEQzZmOatMO"
      },
      "source": [
        "#Defining the configuration file which will be used to perform hyperparameter sweep experiments\n",
        "sweep_config = {\n",
        "    'method': 'grid', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'loss',\n",
        "      'goal': 'minimize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'accumulation_steps': {\n",
        "            'values': [8, 16, 32]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-5,2e-5,3e-5,4e-5,5e-5]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "config_defaults = {\n",
        "        'learning_rate': 0.1,\n",
        "        'accumulation_steps' : 8,\n",
        "        'train_batch_size' : 1,\n",
        "        'eval_batch_size' : 2,\n",
        "        'epochs' : 6\n",
        "\n",
        "    }\n",
        "\n",
        "#Initializing Weights & Biases project for performing hyperparameter sweep experiments\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"nlp_project_sweep_test\")\n",
        "model_list = [] #For storing each the finetuned models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNLWQHYJw_dq"
      },
      "source": [
        "# Use this if you need to clear GPU memory.\n",
        "del model_qa\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZhRn4lQdSI2"
      },
      "source": [
        "#Defining evaluation loss function\n",
        "def eval_loss(eval_loader, model_temp):\n",
        "    model_temp.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in eval_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model_temp(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        wandb.log({\"eval loss\": loss})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBOd-CQvWjXU"
      },
      "source": [
        "#Defining the function which performs finetuning of the Longformer model on the Rulebook dataset, the reference for which will be passed into Weights & Biases for performing hyperparameter sweep experiments\n",
        "def train_eval():\n",
        "  model_qa = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)\n",
        "  model_qa = model_qa.to(device)\n",
        "  torch.manual_seed(123)\n",
        "  wandb.init(config=config_defaults)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=wandb.config.train_batch_size, shuffle=True)\n",
        "  eval_loader = DataLoader(eval_dataset, batch_size=wandb.config.eval_batch_size, shuffle=True)\n",
        "  optim = AdamW(model_qa.parameters(), lr=wandb.config.learning_rate)\n",
        "\n",
        "  wandb.watch(model_qa, log=\"all\")\n",
        "\n",
        "  optim.zero_grad()\n",
        "\n",
        "  for epoch in range(wandb.config.epochs):\n",
        "    print(\"At epoch: \",epoch)\n",
        "    accumulator = 0 # Initialize accumulator for gradient accumulation\n",
        "    model_qa.train()\n",
        "    for batch in train_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      start_positions = batch['start_positions'].to(device)\n",
        "      end_positions = batch['end_positions'].to(device)\n",
        "      outputs = model_qa(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "      loss = outputs[0]\n",
        "      loss = loss / wandb.config.accumulation_steps # Normalize loss\n",
        "      loss.backward()\n",
        "      accumulator += 1\n",
        "      if (accumulator) % wandb.config.accumulation_steps == 0: # If we've finished accumulating gradient, update model and evaluate.\n",
        "        wandb.log({\"training loss\": loss})\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        eval_loss(eval_loader, model_qa) # get evaluation loss\n",
        "\n",
        "  #Computing Average F1 score between the set of predicted test answers and gold rest answers\n",
        "  f1_scores = []\n",
        "  for i in range(len(test_questions)):\n",
        "    current_test_question = test_questions[i]\n",
        "    current_test_context = test_contexts[i]\n",
        "    current_test_answer = test_answers[i]\n",
        "    test_inputs = tokenizer_qa(current_test_questions, current_test_context, return_tensors='pt') # Tokenize the input using the QA model.\n",
        "    with torch.no_grad():\n",
        "      test_inputs = test_inputs.to(device)\n",
        "      answer_start_scores, answer_end_scores = model_qa(**test_inputs) # Get outputs from QA model.\n",
        "      answer_start = torch.argmax(answer_start_scores)\n",
        "      answer_end = torch.argmax(answer_end_scores) + 1\n",
        "      inference_answer = tokenizer_qa.convert_tokens_to_string(tokenizer_qa.convert_ids_to_tokens(test_inputs[\"input_ids\"][0][answer_start:answer_end])).lstrip(' ') # Remove leading whitespace\n",
        "      f1_score = compute_f1(current_test_answer, inference_answer)\n",
        "      f1_scores.append(f1_score)\n",
        "  \n",
        "  average_f1_score = (float)(sum(f1_scores) / len(f1_scores))\n",
        "  wandb.log({\"Average F1 score\": average_f1_score})\n",
        "\n",
        "  \n",
        "  torch.save(model_qa.state_dict(), 'model.h5')\n",
        "  wandb.save('model.h5')\n",
        "\n",
        "  model_list.append(model_qa)\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQZh1iPRgKxz"
      },
      "source": [
        "#Weights and Biases function callback to perform the actual hyperparameter sweep experiment\n",
        "wandb.agent(sweep_id, function=train_eval)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}